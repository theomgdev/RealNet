"""
RealStore: Checkpoint Management Utilities for RealNet.

Provides standard save/load functionality plus "Weight Transplantation" 
for transferring knowledge between models of different sizes.
"""

import torch
import os

def save_checkpoint(model, optimizer, epoch, loss, path, extra_data=None):
    """
    Saves a training checkpoint.
    
    Args:
        model: The RealNet model instance.
        optimizer: The optimizer instance.
        epoch (int): Current epoch number.
        loss (float): Current loss value.
        path (str): File path to save the checkpoint.
        extra_data (dict, optional): Any additional data to save.
    """
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
    }
    
    if extra_data:
        checkpoint.update(extra_data)
        
    # Ensure directory exists
    os.makedirs(os.path.dirname(path), exist_ok=True)
    
    torch.save(checkpoint, path)
    return path


def load_checkpoint(model, optimizer, path, device='cpu', strict=True, lr=None):
    """
    Loads a training checkpoint.
    
    Args:
        model: The RealNet model instance (must match checkpoint architecture if strict=True).
        optimizer: The optimizer instance.
        path (str): File path to the checkpoint.
        device (str): Device to load tensors to.
        strict (bool): If True, raises error on architecture mismatch.
                       If False, ignores mismatched keys (standard PyTorch behavior).
        lr (float, optional): If provided, overwrites the learning rate in the optimizer 
                              after loading the state.
    
    Returns:
        dict: The loaded checkpoint data (epoch, loss, etc.)
        
    Raises:
        FileNotFoundError: If checkpoint file doesn't exist.
        RuntimeError: If strict=True and architecture doesn't match.
    """
    if not os.path.exists(path):
        raise FileNotFoundError(f"Checkpoint not found: {path}")
    
    checkpoint = torch.load(path, map_location=device)
    
    model.load_state_dict(checkpoint['model_state_dict'], strict=strict)
    
    if optimizer and 'optimizer_state_dict' in checkpoint:
        try:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            
            # Allow overwriting LR if requested (e.g. for fine-tuning or experiments)
            if lr is not None:
                for param_group in optimizer.param_groups:
                    param_group['lr'] = lr
                print(f"‚ö° Optimizer LR overwritten to: {lr}")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Could not load optimizer state: {e}. Optimizer will start fresh.")
    
    return checkpoint


def transplant_weights(model, checkpoint_path, device='cpu', verbose=True):
    """
    üß¨ Weight Transplantation: Transfer learned weights from a checkpoint to a model,
    even if the architectures (num_neurons) don't match.
    
    This is useful for:
    - Scaling up: Starting a larger model with knowledge from a smaller one.
    - Scaling down: Compressing a large model into a smaller one.
    - Warm starts: Any learned weights are better than random initialization.
    
    How it works:
    - For each parameter (W, B, LayerNorm), the overlapping region is copied.
    - Non-overlapping regions keep their initialized values.
    
    Args:
        model: The target RealNet model instance (already initialized).
        checkpoint_path (str): Path to the source checkpoint.
        device (str): Device to load tensors to.
        verbose (bool): If True, prints transplant statistics.
    
    Returns:
        dict: Statistics about the transplant operation.
    """
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
    
    checkpoint = torch.load(checkpoint_path, map_location=device)
    source_state = checkpoint.get('model_state_dict', checkpoint) # Support raw state_dict too
    
    target_state = model.state_dict()
    
    stats = {
        'total_params': 0,
        'transplanted_params': 0,
        'new_params': 0,
        'keys_matched': [],
        'keys_resized': [],
        'keys_missing': [],
    }
    
    for key in target_state:
        target_tensor = target_state[key]
        stats['total_params'] += target_tensor.numel()
        
        if key not in source_state:
            stats['keys_missing'].append(key)
            stats['new_params'] += target_tensor.numel()
            continue
            
        source_tensor = source_state[key]
        
        if source_tensor.shape == target_tensor.shape:
            # Perfect match - direct copy
            target_state[key] = source_tensor.clone()
            stats['transplanted_params'] += target_tensor.numel()
            stats['keys_matched'].append(key)
            
        else:
            # Size mismatch - copy overlapping region
            stats['keys_resized'].append((key, source_tensor.shape, target_tensor.shape))
            
            if source_tensor.dim() == 2 and target_tensor.dim() == 2:
                # 2D Tensor (Weights W: N√óN)
                min_rows = min(source_tensor.shape[0], target_tensor.shape[0])
                min_cols = min(source_tensor.shape[1], target_tensor.shape[1])
                target_state[key][:min_rows, :min_cols] = source_tensor[:min_rows, :min_cols]
                stats['transplanted_params'] += min_rows * min_cols
                stats['new_params'] += target_tensor.numel() - (min_rows * min_cols)
                
            elif source_tensor.dim() == 1 and target_tensor.dim() == 1:
                # 1D Tensor (Bias B, LayerNorm weights)
                min_len = min(source_tensor.shape[0], target_tensor.shape[0])
                target_state[key][:min_len] = source_tensor[:min_len]
                stats['transplanted_params'] += min_len
                stats['new_params'] += target_tensor.numel() - min_len
                
            else:
                # Dimension mismatch - skip
                stats['keys_missing'].append(key)
                stats['new_params'] += target_tensor.numel()
    
    # Load the modified state
    model.load_state_dict(target_state)
    
    if verbose:
        print("üß¨ Weight Transplantation Complete!")
        print(f"   Total Parameters: {stats['total_params']:,}")
        print(f"   Transplanted: {stats['transplanted_params']:,} ({100*stats['transplanted_params']/stats['total_params']:.1f}%)")
        print(f"   New (Initialized): {stats['new_params']:,} ({100*stats['new_params']/stats['total_params']:.1f}%)")
        
        if stats['keys_resized']:
            print("   Resized Tensors:")
            for key, src_shape, tgt_shape in stats['keys_resized']:
                print(f"      {key}: {list(src_shape)} ‚Üí {list(tgt_shape)}")
    
    return stats


def get_checkpoint_info(path, device='cpu'):
    """
    Reads checkpoint metadata without loading into a model.
    
    Args:
        path (str): Path to the checkpoint file.
        device (str): Device to load tensors to.
    
    Returns:
        dict: Checkpoint information (epoch, loss, model size, etc.)
    """
    if not os.path.exists(path):
        raise FileNotFoundError(f"Checkpoint not found: {path}")
    
    checkpoint = torch.load(path, map_location=device)
    
    info = {
        'epoch': checkpoint.get('epoch', 'Unknown'),
        'loss': checkpoint.get('loss', 'Unknown'),
        'keys': list(checkpoint.get('model_state_dict', {}).keys()),
    }
    
    # Calculate model size
    if 'model_state_dict' in checkpoint:
        total_params = sum(
            t.numel() for t in checkpoint['model_state_dict'].values()
        )
        info['total_params'] = total_params
        
        # Infer num_neurons from W shape
        if 'W' in checkpoint['model_state_dict']:
            info['num_neurons'] = checkpoint['model_state_dict']['W'].shape[0]
    
    return info
