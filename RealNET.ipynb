{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RealNet-LLM Cloud Trainer\n",
        "- This IPYNB file written to train RealNet based LLM's on Google Collab or Google Cloud, paths kept Collab friendly and root relative, AI checkpoints loaded/saved from Google Drive. Settings(neuron_count, sequence length etc.) kept high for infinite cloud training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mqIrBWpUp5Hu",
        "outputId": "766549a0-03cb-4170-ce45-e31653c8214c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'RealNet'...\n",
            "remote: Enumerating objects: 1162, done.\u001b[K\n",
            "remote: Counting objects: 100% (274/274), done.\u001b[K\n",
            "remote: Compressing objects: 100% (163/163), done.\u001b[K\n",
            "remote: Total 1162 (delta 195), reused 178 (delta 108), pack-reused 888 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1162/1162), 360.45 KiB | 1.47 MiB/s, done.\n",
            "Resolving deltas: 100% (767/767), done.\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from -r RealNet/requirements.txt (line 1)) (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r RealNet/requirements.txt (line 2)) (2.0.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from -r RealNet/requirements.txt (line 3)) (4.0.0)\n",
            "Collecting bitsandbytes (from -r RealNet/requirements.txt (line 4))\n",
            "  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r RealNet/requirements.txt (line 5)) (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r RealNet/requirements.txt (line 6)) (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from -r RealNet/requirements.txt (line 7)) (2.32.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r RealNet/requirements.txt (line 8)) (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r RealNet/requirements.txt (line 9)) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r RealNet/requirements.txt (line 1)) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r RealNet/requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r RealNet/requirements.txt (line 1)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r RealNet/requirements.txt (line 1)) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r RealNet/requirements.txt (line 1)) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r RealNet/requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r RealNet/requirements.txt (line 1)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r RealNet/requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r RealNet/requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r RealNet/requirements.txt (line 1)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r RealNet/requirements.txt (line 1)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r RealNet/requirements.txt (line 1)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r RealNet/requirements.txt (line 1)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r RealNet/requirements.txt (line 1)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r RealNet/requirements.txt (line 1)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r RealNet/requirements.txt (line 1)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r RealNet/requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r RealNet/requirements.txt (line 1)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r RealNet/requirements.txt (line 1)) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r RealNet/requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r RealNet/requirements.txt (line 1)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r RealNet/requirements.txt (line 1)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r RealNet/requirements.txt (line 1)) (3.5.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r RealNet/requirements.txt (line 3)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r RealNet/requirements.txt (line 3)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->-r RealNet/requirements.txt (line 3)) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->-r RealNet/requirements.txt (line 3)) (0.70.16)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r RealNet/requirements.txt (line 3)) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets->-r RealNet/requirements.txt (line 3)) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets->-r RealNet/requirements.txt (line 3)) (6.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r RealNet/requirements.txt (line 5)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r RealNet/requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r RealNet/requirements.txt (line 5)) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r RealNet/requirements.txt (line 5)) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r RealNet/requirements.txt (line 5)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r RealNet/requirements.txt (line 5)) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r RealNet/requirements.txt (line 5)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r RealNet/requirements.txt (line 6)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r RealNet/requirements.txt (line 6)) (2025.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->-r RealNet/requirements.txt (line 7)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->-r RealNet/requirements.txt (line 7)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->-r RealNet/requirements.txt (line 7)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->-r RealNet/requirements.txt (line 7)) (2026.1.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r RealNet/requirements.txt (line 8)) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r RealNet/requirements.txt (line 8)) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r RealNet/requirements.txt (line 8)) (3.6.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r RealNet/requirements.txt (line 3)) (3.13.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets->-r RealNet/requirements.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->-r RealNet/requirements.txt (line 5)) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->-r RealNet/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->-r RealNet/requirements.txt (line 1)) (3.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r RealNet/requirements.txt (line 3)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r RealNet/requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r RealNet/requirements.txt (line 3)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r RealNet/requirements.txt (line 3)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r RealNet/requirements.txt (line 3)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r RealNet/requirements.txt (line 3)) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r RealNet/requirements.txt (line 3)) (1.22.0)\n",
            "Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.49.1\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/theomgdev/RealNet.git\n",
        "!pip install -r RealNet/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "738c17dc",
        "outputId": "00300c43-7a98-46db-ec7b-63e592149cd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b91b9b0026d64f838c5bb739f4544105",
            "5fe44d116771413086fb53d506c66ae3",
            "b4f9197effdd4b309f4642c4d7790226",
            "27bddea9e66e447bad34eed9d9af84e9",
            "3cba134770e14193b57a4b03a6879ace",
            "9df4e339a22d492f9a4f4967affdbf97",
            "46b7e7e6516243a99be1ec6b3ecc28ae",
            "edf9dedf22bd451097dee8d13827f024",
            "6dbbe4a819ef48598ab9a07cd1a3bf54",
            "4480b16cb46242d68dd98d5fb13355d6",
            "b2dba098822d406a94971f039bec6eea",
            "ebe90f94272341028a42036b48718e9c",
            "1e3ec207041b4ee09557f5027401e4a6",
            "efe5028be49745f49ac232357608b09f",
            "d7e8c88f568440e997a795badb70657c",
            "b3b402f5c0d444c5a1e900c765703188",
            "4e48976ffbd1499c929d2430f9f5d36d",
            "a967241441664c6a894833737472068c",
            "24aeaa3ab0fc4110a839eca4ad2bdd8f",
            "1d5c34f495834524a1a077b67fa35ee7",
            "f0e3cba64b0147a9bcbc56961f48727e",
            "42e01134b61948508d8fdeb540ab3a07",
            "f5b63ee58f744648908003d21033e61b",
            "6e76caf009a249bf9d8316329b7e2518",
            "e1887135234b4cf4915691160febfa82",
            "7b76f4aee386460bab874dcde2c68731",
            "b233baea8150447daf079c651075c504",
            "f11cb5c9dc474353b17c776372af38f0",
            "bd22b2a7616f4c9e9463dc1ab0f7bd6e",
            "0aca8bf099704960a33f8b1ef88bea83",
            "66510255473747c39f0ca25c949889cb",
            "d2af97eeb1124f248c6a099803e7ff86",
            "001564ca5bad41ffa72454dabaf10015"
          ]
        },
        "id": "-jHEoQ1GsKQh",
        "outputId": "3d3fd29c-7bf8-4c92-ebed-c7a3cf53d1e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  _C._set_float32_matmul_precision(precision)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ RealNet-1B (FineWeb Streaming)...\n",
            "--- Configuration ---\n",
            "SEQ_LEN: 4096\n",
            "BATCH_SIZE: -1 (Will Auto-Tune if -1)\n",
            "NUM_NEURONS: 4096\n",
            "TRUNCATED_BPTT_STEPS: 64\n",
            "STEPS_PER_EPOCH: 10\n",
            "LOG_INTERVAL: 1\n",
            "MAX_START_SKIP: 1000\n",
            "RESET_DATA_ITER: False\n",
            "GENERATION_LENGTH: 1024\n",
            "THINK_GAP: 5\n",
            "ACTIVATION: gelu\n",
            "VOCAB_SIZE: 256\n",
            "DEVICE: cuda\n",
            "NEUROGENESIS: Enabled=False, MaxLossInc=10, Amount=10\n",
            "PHOENIX (Regeneration): Mode=percentage, Val=0.1%, Interval=10\n",
            "RESET_OPTIM_ON_LOAD: False\n",
            "SCHEDULER: Enabled=True, T0=100, MinLR=1e-07\n",
            "LEARNING_RATE: 1e-06\n",
            "OVERWRITE_LR_OF_CKPT: True\n",
            "---------------------\n",
            "üìÇ Resuming dataset from index: 487100\n",
            "üåä Connecting to FineWeb-Edu (CC-MAIN-2024-10)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b91b9b0026d64f838c5bb739f4544105",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebe90f94272341028a42036b48718e9c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/2410 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5b63ee58f744648908003d21033e61b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RealNetTrainer: Using bitsandbytes 8-bit AdamW for VRAM efficiency.\n",
            "\n",
            "‚öñÔ∏è  Auto-Tuning Batch Size...\n",
            "   VRAM Total: 23.80 GB\n",
            "   VRAM Free:  23.66 GB (Allocated: 0.13 GB)\n",
            "   Est. Memory/Sample: 7.86 MB\n",
            "   Optimal Batch Size: 3008\n",
            "Input IDs: 0-255\n",
            "Output IDs: 256-511\n",
            "üîÑ Loading Checkpoint from /content/drive/MyDrive/RealNet/ckpt/llm_fineweb_gelu_latest.pth...\n",
            "‚ö° Optimizer LR overwritten to: 1e-06\n",
            "‚úÖ Resuming from Epoch 28\n",
            "\n",
            "--- GENERATION PREVIEW ---\n",
            "Sample: The meaning of life isues, and an article about contracts including Asian subspecies in Madagascar on March 11, 2015\n",
            "Abstr\n",
            "\n",
            "--- TRAINING LOOP ---\n",
            "üèÜ Historical Best Loss: 1.1044\n",
            "‚è© Resuming from Document #487100...\n",
            "Epoch 28 | Batch 0 | Doc #489534 | Loss 1.1030 | PPL 3.01 | LR 1.00e-06\n",
            "Epoch 28 | Batch 1 | Doc #491902 | Loss 1.1080 | PPL 3.03 | LR 9.99e-07\n",
            "Epoch 28 | Batch 2 | Doc #494212 | Loss 1.1081 | PPL 3.03 | LR 9.98e-07\n",
            "Epoch 28 | Batch 3 | Doc #496566 | Loss 1.0989 | PPL 3.00 | LR 9.96e-07\n",
            "Epoch 28 | Batch 4 | Doc #498961 | Loss 1.0992 | PPL 3.00 | LR 9.94e-07\n",
            "Epoch 28 | Batch 5 | Doc #501359 | Loss 1.1034 | PPL 3.01 | LR 9.92e-07\n",
            "Epoch 28 | Batch 6 | Doc #503705 | Loss 1.1045 | PPL 3.02 | LR 9.89e-07\n",
            "Epoch 28 | Batch 7 | Doc #506108 | Loss 1.1004 | PPL 3.01 | LR 9.86e-07\n",
            "Epoch 28 | Batch 8 | Doc #508509 | Loss 1.0980 | PPL 3.00 | LR 9.82e-07\n",
            "Epoch 28 | Batch 9 | Doc #510850 | Loss 1.0919 | PPL 2.98 | LR 9.78e-07\n",
            "Epoch 28 Completed | Avg Loss: 1.1015 | Avg PPL 3.01 | Time: 4535.5s\n",
            "--- GENERATION ---\n",
            "The meaning of life is some were many things and create delicious and varied and properly determined to ask if there is an increased risk of developing cardiovascular diseases. The seasons for power which can be propagated by the state and are of interest to speed of length of the medication or other related research, a demonstration of the social media platform for analysis that combines the same measurement in web development process and the impact of muscle cramping.\n",
            "Heart disease, and more bandwidth and the external discrimination and health, the discovery of the fact that only a comparatively easy way to comment on the standard of research is a strategic location for their mothers and girls and teachers, and seek to check what the reasons that the board of payment for the test results is the process of deciding what to do about your problems with the proper safety measures to support each other with control of the digital age. Additionally, a power outage and return on the impact of stress and prevent the inflammatory compound\n",
            "------------------\n",
            "üíæ Checkpoint Saved: /content/drive/MyDrive/RealNet/ckpt/llm_fineweb_gelu_latest.pth (Doc Index: 510850)\n",
            "üèÜ NEW RECORD! Saved: /content/drive/MyDrive/RealNet/ckpt/llm_fineweb_gelu_best.pth (Loss: 1.1015)\n",
            "Epoch 29 | Batch 0 | Doc #513368 | Loss 1.0973 | PPL 3.00 | LR 9.73e-07\n",
            "Epoch 29 | Batch 1 | Doc #515773 | Loss 1.1067 | PPL 3.02 | LR 9.68e-07\n",
            "Epoch 29 | Batch 2 | Doc #518055 | Loss 1.0996 | PPL 3.00 | LR 9.63e-07\n",
            "Epoch 29 | Batch 3 | Doc #520416 | Loss 1.1091 | PPL 3.03 | LR 9.57e-07\n",
            "Epoch 29 | Batch 4 | Doc #522755 | Loss 1.1001 | PPL 3.00 | LR 9.51e-07\n",
            "Epoch 29 | Batch 5 | Doc #525002 | Loss 1.1034 | PPL 3.01 | LR 9.44e-07\n",
            "Epoch 29 | Batch 6 | Doc #527420 | Loss 1.0936 | PPL 2.98 | LR 9.37e-07\n",
            "Epoch 29 | Batch 7 | Doc #529772 | Loss 1.1090 | PPL 3.03 | LR 9.30e-07\n",
            "Epoch 29 | Batch 8 | Doc #532240 | Loss 1.0979 | PPL 3.00 | LR 9.22e-07\n",
            "Epoch 29 | Batch 9 | Doc #534634 | Loss 1.1026 | PPL 3.01 | LR 9.14e-07\n",
            "Epoch 29 Completed | Avg Loss: 1.1019 | Avg PPL 3.01 | Time: 4476.0s\n",
            "--- GENERATION ---\n",
            "The meaning of life is which can be traced back to 1600. The main organism was based on the fact that instead of the second phase showed that the company has developed a fantastic form of supply chain disruption that will be developed to evaluate the release of contraction and control over the continent. The legend of God, who was an American and Asian states. However, this is a good idea to keep in mind that the machine that we take it away. However, they refer to the statement below, I will discuss what we are welcomed and the minimum wage is entirely suspicious in reading someone in a warm disk on the map, the internet to make informed choices based on the situation.\n",
            "The use of these components must be used for a number of things that is also known to be divided into three parts and a theoretical structure and depth.\n",
            "Professor Hamilton was released in the 16th century, indigenous communities. The design of the speech and actions is to have fun with it, you have to be the best option for children ages 14 to 32. In other words, th\n",
            "------------------\n",
            "üíæ Checkpoint Saved: /content/drive/MyDrive/RealNet/ckpt/llm_fineweb_gelu_latest.pth (Doc Index: 534634)\n",
            "Epoch 30 | Batch 0 | Doc #537028 | Loss 1.1016 | PPL 3.01 | LR 9.06e-07\n",
            "Epoch 30 | Batch 1 | Doc #539547 | Loss 1.0994 | PPL 3.00 | LR 8.97e-07\n",
            "Epoch 30 | Batch 2 | Doc #541967 | Loss 1.1046 | PPL 3.02 | LR 8.88e-07\n",
            "Epoch 30 | Batch 3 | Doc #544324 | Loss 1.0950 | PPL 2.99 | LR 8.78e-07\n",
            "Epoch 30 | Batch 4 | Doc #546876 | Loss 1.1004 | PPL 3.01 | LR 8.68e-07\n",
            "Epoch 30 | Batch 5 | Doc #549325 | Loss 1.1122 | PPL 3.04 | LR 8.58e-07\n",
            "Epoch 30 | Batch 6 | Doc #551658 | Loss 1.1048 | PPL 3.02 | LR 8.48e-07\n",
            "Epoch 30 | Batch 7 | Doc #554114 | Loss 1.0992 | PPL 3.00 | LR 8.37e-07\n",
            "Epoch 30 | Batch 8 | Doc #556489 | Loss 1.0970 | PPL 3.00 | LR 8.26e-07\n",
            "Epoch 30 | Batch 9 | Doc #558880 | Loss 1.0928 | PPL 2.98 | LR 8.15e-07\n",
            "Epoch 30 Completed | Avg Loss: 1.1007 | Avg PPL 3.01 | Time: 4504.2s\n",
            "--- GENERATION ---\n",
            "The meaning of life is or shoulder in the world.\n",
            "In conclusion, it can be as simple as it is being in sight to the flower is an influence on the factories in the 1890s. This assessment of female appliance or services and analysis, and research and innovation in social connections with everyone in the network. In the late 19th century, one in the following day a certain extent, the federal government also had a significant influence on the body to grow. When possible or even with the energy and finding a new state of many processes and software can help provide resources and techniques to improve your emotional resilience. While most the inner strength in the same principle of life, on the other hand, are a crucial component of the schools as a tool for transparency is key. Consider the following:\n",
            "- Perform websites for the students in life, and they have shown that they were still in its source is all the same thing as soon as possible.\n",
            "By following these tips for businesses can learn from the past to hold your position in the worl\n",
            "------------------\n",
            "üíæ Checkpoint Saved: /content/drive/MyDrive/RealNet/ckpt/llm_fineweb_gelu_latest.pth (Doc Index: 558880)\n",
            "üèÜ NEW RECORD! Saved: /content/drive/MyDrive/RealNet/ckpt/llm_fineweb_gelu_best.pth (Loss: 1.1007)\n",
            "üî• Phoenix Protocol: Checking for dead synapses...\n",
            "üî• Reborn: 16778/16777216 (0.10%) synapses regenerated.\n",
            "Epoch 31 | Batch 0 | Doc #561198 | Loss 1.9045 | PPL 6.72 | LR 8.03e-07\n",
            "Epoch 31 | Batch 1 | Doc #563607 | Loss 1.1533 | PPL 3.17 | LR 7.91e-07\n",
            "Epoch 31 | Batch 2 | Doc #566029 | Loss 1.1157 | PPL 3.05 | LR 7.79e-07\n",
            "Epoch 31 | Batch 3 | Doc #568377 | Loss 1.1216 | PPL 3.07 | LR 7.67e-07\n",
            "Epoch 31 | Batch 4 | Doc #570826 | Loss 1.1062 | PPL 3.02 | LR 7.54e-07\n",
            "Epoch 31 | Batch 5 | Doc #573223 | Loss 1.1106 | PPL 3.04 | LR 7.42e-07\n",
            "Epoch 31 | Batch 6 | Doc #575558 | Loss 1.1182 | PPL 3.06 | LR 7.29e-07\n",
            "Epoch 31 | Batch 7 | Doc #577912 | Loss 1.1040 | PPL 3.02 | LR 7.16e-07\n",
            "Epoch 31 | Batch 8 | Doc #580249 | Loss 1.1083 | PPL 3.03 | LR 7.02e-07\n",
            "Epoch 31 | Batch 9 | Doc #582524 | Loss 1.1269 | PPL 3.09 | LR 6.89e-07\n",
            "Epoch 31 Completed | Avg Loss: 1.1969 | Avg PPL 3.31 | Time: 4500.9s\n",
            "--- GENERATION ---\n",
            "The meaning of life is randomised interventions within the study.\n",
            "- Students have also been used by both children may be related to completion of the realm of authority at the time. The first sign of persistent illnesses, such as reading a few more days, here are some strategies for treating skin conditions that are considered a clear blow. The result is a similar story.\n",
            "- Discuss how you can make a roadmap and the environment. But in the past, did you know that there are also causes of disease or a pain in the file size than their expertise in the preservation of democracy, the researchers created a brief analysis of the same dog who has little impact on cardiovascular health, and reduce the risk of breast cancer, which is crucial for overall health and well-being.\n",
            "Benefits of Responsive Literacy (CTE) and in response to the specific type of cryptocurrencies, and to track your electronic manufacturing capabilities or characteristics of particles of sewage was conducted in the 1960s. The findings are somewhat small in pressure sour\n",
            "------------------\n",
            "üíæ Checkpoint Saved: /content/drive/MyDrive/RealNet/ckpt/llm_fineweb_gelu_latest.pth (Doc Index: 582524)\n",
            "Epoch 32 | Batch 0 | Doc #584979 | Loss 1.1047 | PPL 3.02 | LR 6.76e-07\n",
            "Epoch 32 | Batch 1 | Doc #587413 | Loss 1.1033 | PPL 3.01 | LR 6.62e-07\n",
            "Epoch 32 | Batch 2 | Doc #589816 | Loss 1.0964 | PPL 2.99 | LR 6.48e-07\n",
            "Epoch 32 | Batch 3 | Doc #592283 | Loss 1.1023 | PPL 3.01 | LR 6.34e-07\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1397056143.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1397056143.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    489\u001b[0m                     \u001b[0my_chunk_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_dilated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mt_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m                     loss, current_state = trainer.train_batch(\n\u001b[0m\u001b[1;32m    492\u001b[0m                         \u001b[0mx_chunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m                         \u001b[0my_chunk_flat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/RealNet/realnet/training/trainer.py\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, input_features, target_values, thinking_steps, gradient_accumulation_steps, full_sequence, mask, output_transform, initial_state, return_state)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m         )\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    355\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    354\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    355\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "\n",
        "# --- ENVIRONMENT & IMPORTS ---\n",
        "from RealNet.realnet import RealNet, RealNetTrainer, save_checkpoint, load_checkpoint, transplant_weights\n",
        "\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "TRUNCATED_BPTT_SEQ_LEN = 10\n",
        "GENERATION_LENGTH = 1024\n",
        "SEQ_LEN = 4096\n",
        "BATCH_SIZE = -1\n",
        "STEPS_PER_EPOCH = 10\n",
        "LOG_INTERVAL = 1\n",
        "MAX_START_SKIP = 1000\n",
        "RESET_DATA_ITER = False\n",
        "NUM_NEURONS = 4096\n",
        "ACTIVATION = 'gelu'\n",
        "THINK_GAP = 5\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# NEUROGENESIS CONFIG\n",
        "NEUROGENESIS_ENABLED = False\n",
        "MAX_LOSS_INCREASE = 10\n",
        "NEUROGENESIS_AMOUNT = 10\n",
        "\n",
        "# REGENERATION CONFIG (PHOENIX)\n",
        "DARWINIAN_REGENERATION = True\n",
        "REGENERATION_MODE = 'percentage' # 'threshold' or 'percentage'\n",
        "REGENERATION_THRESHOLD = 0.01\n",
        "REGENERATION_PERCENTAGE = 0.001\n",
        "REGENERATION_INTERVAL = 10 # Epochs between regeneration checks\n",
        "\n",
        "# OPTIMIZER CONFIG\n",
        "VOCAB_SIZE = 256\n",
        "RESET_OPTIMIZER_ON_LOAD = False\n",
        "OVERWRITE_LR_OF_CKPT = True\n",
        "LEARNING_RATE = 1e-6\n",
        "\n",
        "# SCHEDULER CONFIG\n",
        "USE_SCHEDULER = True\n",
        "SCHEDULER_T0 = 100\n",
        "SCHEDULER_ETA_MIN = 1e-7 \n",
        "\n",
        "CHAR_TO_IDX = {i: i for i in range(256)}\n",
        "IDX_TO_CHAR = {i: i for i in range(256)}\n",
        "\n",
        "# --- DATASET ---\n",
        "from datasets import load_dataset\n",
        "\n",
        "class FineWebIterableDataset(torch.utils.data.IterableDataset):\n",
        "    def __init__(self, seq_len, skip_offset=0, debug=False):\n",
        "        self.seq_len = seq_len\n",
        "        self.skip_offset = skip_offset\n",
        "        self.debug = debug\n",
        "        self.current_doc_index = skip_offset # Initialize to avoid AttributeError in main\n",
        "        print(\"üåä Connecting to FineWeb-Edu (CC-MAIN-2024-10)...\")\n",
        "        self.dataset = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=\"CC-MAIN-2024-10\", split=\"train\", streaming=True)\n",
        "\n",
        "    def __iter__(self):\n",
        "        start_skip = self.skip_offset\n",
        "\n",
        "        if start_skip == 0 or RESET_DATA_ITER:\n",
        "             start_skip = random.randint(0, MAX_START_SKIP)\n",
        "             print(f\"üîÄ Random Start: Skipping {start_skip} documents...\")\n",
        "        else:\n",
        "             print(f\"‚è© Resuming from Document #{start_skip}...\")\n",
        "\n",
        "        # Worker-local index tracking\n",
        "        local_doc_index = start_skip\n",
        "\n",
        "        if start_skip > 0:\n",
        "            iterator = iter(self.dataset.skip(start_skip))\n",
        "        else:\n",
        "            iterator = iter(self.dataset)\n",
        "\n",
        "        buffer_bytes = b\"\"\n",
        "\n",
        "        while True:\n",
        "            # Replenish buffer\n",
        "            while len(buffer_bytes) < self.seq_len + 1:\n",
        "                try:\n",
        "                    item = next(iterator)\n",
        "                    local_doc_index += 1\n",
        "                    if self.debug and local_doc_index % 1000 == 0:\n",
        "                        print(f\"üìä Streaming Index: Document #{local_doc_index}\")\n",
        "                    text = item.get('text', '')\n",
        "                    new_bytes = text.encode('utf-8', errors='replace') + b\" \"\n",
        "                    buffer_bytes += new_bytes\n",
        "                except StopIteration:\n",
        "                    iterator = iter(self.dataset)\n",
        "                    local_doc_index = 0\n",
        "\n",
        "            # Extract chunk\n",
        "            chunk_bytes = buffer_bytes[:self.seq_len + 1]\n",
        "            buffer_bytes = buffer_bytes[self.seq_len + 1:]\n",
        "\n",
        "            indices = list(chunk_bytes)\n",
        "\n",
        "            if len(indices) == self.seq_len + 1:\n",
        "                x = torch.tensor(indices[:-1], dtype=torch.long)\n",
        "                y = torch.tensor(indices[1:], dtype=torch.long)\n",
        "                # Yield index too so main process knows where we are\n",
        "                yield x, y, local_doc_index\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return VOCAB_SIZE\n",
        "\n",
        "    @property\n",
        "    def char_to_idx(self):\n",
        "        return CHAR_TO_IDX\n",
        "\n",
        "    @property\n",
        "    def idx_to_char(self):\n",
        "        return IDX_TO_CHAR\n",
        "\n",
        "def generate(model, dataset, start_str=\"The\", length=None, temperature=0.8, top_k=40, top_p=0.9):\n",
        "    if length is None:\n",
        "        length = GENERATION_LENGTH\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    input_bytes = start_str.encode('utf-8', errors='replace')\n",
        "    input_seq = list(input_bytes)\n",
        "\n",
        "    current_state = None\n",
        "\n",
        "    # Warm up state (Native Thinking)\n",
        "    # We send raw tokens, but ask model to think for (Gap+1) steps per token\n",
        "    x_in = torch.tensor(input_seq, dtype=torch.long, device=model.device).unsqueeze(0)\n",
        "    steps_total = x_in.shape[1] * (THINK_GAP + 1)\n",
        "    \n",
        "    generated_bytes = bytearray(input_bytes)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _, current_state = model(x_in, steps=steps_total)\n",
        "\n",
        "        last_byte_idx = input_seq[-1]\n",
        "\n",
        "        for _ in range(length):\n",
        "            # Native Single Step Generation\n",
        "            # Input: 1 Token. Steps: 1 + Gap.\n",
        "            total_step_single = 1 + THINK_GAP\n",
        "\n",
        "            x_next = torch.tensor([[last_byte_idx]], dtype=torch.long, device=model.device)\n",
        "\n",
        "            preds, current_state = model(x_next, steps=total_step_single, current_state=current_state)\n",
        "            \n",
        "            # Prediction is at the END of the thinking block\n",
        "            # Preds shape: (Batch, 1, Output) in native smart output mode\n",
        "            logits = preds[0, 0, model.output_ids]\n",
        "\n",
        "            # Sampling logic\n",
        "            if temperature > 0:\n",
        "                logits = logits / temperature\n",
        "\n",
        "            if top_k is not None and top_k > 0:\n",
        "                v, _ = torch.topk(logits, min(top_k, len(logits)))\n",
        "                logits[logits < v[-1]] = float('-inf')\n",
        "\n",
        "            if top_p is not None and top_p > 0 and top_p < 1.0:\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                indices_to_remove = sorted_indices_to_remove.scatter(0, sorted_indices, sorted_indices_to_remove)\n",
        "                logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "            probs = torch.softmax(logits, dim=0)\n",
        "\n",
        "            if torch.isnan(probs).any() or torch.sum(probs) == 0:\n",
        "                probs = torch.ones_like(probs) / len(probs)\n",
        "\n",
        "            next_idx = torch.multinomial(probs, 1).item()\n",
        "\n",
        "            generated_bytes.append(next_idx)\n",
        "            last_byte_idx = next_idx\n",
        "\n",
        "    try:\n",
        "        return generated_bytes.decode('utf-8', errors='replace')\n",
        "    except:\n",
        "        return str(generated_bytes)\n",
        "\n",
        "def initialize_system(vocab_size, num_neurons, device, lr=1e-4, activation='gelu'):\n",
        "    input_ids = list(range(vocab_size))\n",
        "    output_ids = list(range(vocab_size, 2 * vocab_size))\n",
        "\n",
        "    model = RealNet(\n",
        "        num_neurons=num_neurons,\n",
        "        input_ids=input_ids,\n",
        "        output_ids=output_ids,\n",
        "        device=device,\n",
        "        dropout_rate=0.0,\n",
        "        activation=activation,\n",
        "        weight_init='orthogonal',\n",
        "        gradient_checkpointing=True\n",
        "    )\n",
        "\n",
        "    trainer = RealNetTrainer(model, lr=lr, device=device, gradient_persistence=0.0, synaptic_noise=0)\n",
        "\n",
        "    return model, trainer, input_ids, output_ids\n",
        "\n",
        "def calculate_optimal_batch_size(device, num_neurons, activation, seq_len, think_gap, truncated_bptt_seq_len):\n",
        "    \"\"\"Calculates optimal batch size based on VRAM capacity.\"\"\"\n",
        "    print(\"\\n‚öñÔ∏è  Auto-Tuning Batch Size...\")\n",
        "\n",
        "    if device == 'cpu':\n",
        "        return 32\n",
        "\n",
        "    if device == 'cuda':\n",
        "        t = torch.cuda.get_device_properties(0).total_memory\n",
        "        a = torch.cuda.memory_allocated(0)\n",
        "        free_vram = t - a\n",
        "\n",
        "        print(f\"   VRAM Total: {t / 1e9:.2f} GB\")\n",
        "        print(f\"   VRAM Free:  {free_vram / 1e9:.2f} GB (Allocated: {a / 1e9:.2f} GB)\")\n",
        "\n",
        "        # Heuristic: Bytes per neuron per step (FP16 Activations + Grads + Overhead)\n",
        "        # Native Mode: We only store activations for (Batch, SeqLen) now! (Outputs are decimated)\n",
        "        # However, internal gradients still track through time.\n",
        "        BYTES_PER_NEURON_STEP = 5\n",
        "\n",
        "        if activation == 'swiglu':\n",
        "            BYTES_PER_NEURON_STEP *= 1.5\n",
        "\n",
        "        if truncated_bptt_seq_len > 0:\n",
        "            # We process raw tokens but computation graph is deep\n",
        "            effective_mem_len = truncated_bptt_seq_len * (think_gap + 1)\n",
        "        else:\n",
        "            effective_mem_len = seq_len * (think_gap + 1)\n",
        "\n",
        "        mem_per_sample = effective_mem_len * num_neurons * BYTES_PER_NEURON_STEP\n",
        "        safe_vram = free_vram * 0.85\n",
        "\n",
        "        calc_batch = int(safe_vram / mem_per_sample) if mem_per_sample > 0 else 1\n",
        "        calc_batch = max(1, calc_batch)\n",
        "\n",
        "        if calc_batch > 8:\n",
        "            calc_batch = (calc_batch // 8) * 8\n",
        "\n",
        "        print(f\"   Est. Memory/Sample: {mem_per_sample / 1e6:.2f} MB\")\n",
        "        print(f\"   Optimal Batch Size: {calc_batch}\")\n",
        "\n",
        "        return calc_batch\n",
        "    return 32\n",
        "\n",
        "def main():\n",
        "    global NUM_NEURONS, BATCH_SIZE # Allow updating global config if needed\n",
        "\n",
        "    print(f\"üöÄ RealNet-1B (FineWeb Streaming) - NATIVE THINKING MODE\")\n",
        "    print(f\"--- Configuration ---\")\n",
        "    print(f\"SEQ_LEN: {SEQ_LEN}\")\n",
        "    print(f\"BATCH_SIZE: {BATCH_SIZE} (Will Auto-Tune if -1)\")\n",
        "    print(f\"NUM_NEURONS: {NUM_NEURONS}\")\n",
        "    print(f\"TRUNCATED_BPTT_SEQ_LEN (Tokens): {TRUNCATED_BPTT_SEQ_LEN}\")\n",
        "    print(f\"STEPS_PER_EPOCH: {STEPS_PER_EPOCH}\")\n",
        "    print(f\"LOG_INTERVAL: {LOG_INTERVAL}\")\n",
        "    print(f\"MAX_START_SKIP: {MAX_START_SKIP}\")\n",
        "    print(f\"RESET_DATA_ITER: {RESET_DATA_ITER}\")\n",
        "    print(f\"GENERATION_LENGTH: {GENERATION_LENGTH}\")\n",
        "    print(f\"THINK_GAP: {THINK_GAP}\")\n",
        "    print(f\"ACTIVATION: {ACTIVATION}\")\n",
        "    print(f\"VOCAB_SIZE: {VOCAB_SIZE}\")\n",
        "    print(f\"DEVICE: {DEVICE}\")\n",
        "    print(f\"NEUROGENESIS: Enabled={NEUROGENESIS_ENABLED}, MaxLossInc={MAX_LOSS_INCREASE}, Amount={NEUROGENESIS_AMOUNT}\")\n",
        "    if DARWINIAN_REGENERATION:\n",
        "        regen_val = f\"{REGENERATION_PERCENTAGE:.1%}\" if REGENERATION_MODE == 'percentage' else f\"{REGENERATION_THRESHOLD}\"\n",
        "        print(f\"PHOENIX (Regeneration): Mode={REGENERATION_MODE}, Val={regen_val}, Interval={REGENERATION_INTERVAL}\")\n",
        "    else:\n",
        "        print(f\"PHOENIX (Regeneration): Disabled\")\n",
        "    print(f\"RESET_OPTIM_ON_LOAD: {RESET_OPTIMIZER_ON_LOAD}\")\n",
        "    print(f\"SCHEDULER: Enabled={USE_SCHEDULER}, T0={SCHEDULER_T0}, MinLR={SCHEDULER_ETA_MIN}\")\n",
        "    print(f\"LEARNING_RATE: {LEARNING_RATE}\")\n",
        "    print(f\"OVERWRITE_LR_OF_CKPT: {OVERWRITE_LR_OF_CKPT}\")\n",
        "    print(f\"---------------------\")\n",
        "\n",
        "    # --- CHECKPOINT PRE-LOAD (For Data Resume) ---\n",
        "    CKPT_DIR = os.path.join('/content/drive/MyDrive/RealNet', 'ckpt')\n",
        "    os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "    CKPT_PATH = os.path.join(CKPT_DIR, f'llm_fineweb_{ACTIVATION}_latest.pth')\n",
        "    CKPT_BEST_PATH = os.path.join(CKPT_DIR, f'llm_fineweb_{ACTIVATION}_best.pth')\n",
        "\n",
        "    resume_doc_index = 0\n",
        "    start_epoch = 0\n",
        "\n",
        "    # --- CHECKPOINT PRE-LOAD (Dataset Resume) ---\n",
        "    if os.path.exists(CKPT_PATH):\n",
        "        try:\n",
        "             peek = torch.load(CKPT_PATH, map_location='cpu')\n",
        "             resume_doc_index = peek.get('dataset_step', 0)\n",
        "             if resume_doc_index > 0:\n",
        "                 print(f\"üìÇ Resuming dataset from index: {resume_doc_index}\")\n",
        "\n",
        "             start_epoch = peek.get('epoch', -1) + 1\n",
        "        except:\n",
        "             pass\n",
        "\n",
        "    dataset = FineWebIterableDataset(SEQ_LEN, skip_offset=resume_doc_index, debug=False)\n",
        "\n",
        "    # --- MODEL SETUP ---\n",
        "    model, trainer, input_ids, output_ids = initialize_system(dataset.get_vocab_size(), NUM_NEURONS, DEVICE, LEARNING_RATE, ACTIVATION)\n",
        "    NUM_NEURONS = model.num_neurons\n",
        "\n",
        "    # --- BATCH SIZE OPTIMIZATION ---\n",
        "    if BATCH_SIZE == -1:\n",
        "         BATCH_SIZE = calculate_optimal_batch_size(\n",
        "             DEVICE,\n",
        "             NUM_NEURONS,\n",
        "             ACTIVATION,\n",
        "             SEQ_LEN,\n",
        "             THINK_GAP,\n",
        "             TRUNCATED_BPTT_SEQ_LEN\n",
        "         )\n",
        "\n",
        "    # DataLoader for IterableDataset\n",
        "    # Streaming with 1 worker allows background downloading without duplication\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=1,        # 1 Background process for downloading\n",
        "        prefetch_factor=4,    # Buffer 4 batches ahead in RAM\n",
        "        persistent_workers=True, # Keep connection alive\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    print(f\"Input IDs: {input_ids[0]}-{input_ids[-1]}\")\n",
        "    print(f\"Output IDs: {output_ids[0]}-{output_ids[-1]}\")\n",
        "\n",
        "    # --- CHECKPOINT LOADING (Full) ---\n",
        "    if os.path.exists(CKPT_PATH):\n",
        "        # Pre-check dimensions to handle mismatches interactively\n",
        "        try:\n",
        "            ckpt_peek = torch.load(CKPT_PATH, map_location=DEVICE)\n",
        "            if 'model_state_dict' in ckpt_peek and 'W' in ckpt_peek['model_state_dict']:\n",
        "                saved_dim = ckpt_peek['model_state_dict']['W'].shape[0]\n",
        "\n",
        "                if saved_dim != NUM_NEURONS:\n",
        "                    print(f\"\\n‚ö†Ô∏è ARCHITECTURE MISMATCH DETECTED!\")\n",
        "                    print(f\"   Current Model: {NUM_NEURONS}\")\n",
        "                    print(f\"   Checkpoint:    {saved_dim}\")\n",
        "\n",
        "                    print(\"Select action:\")\n",
        "                    print(\"[1] Resize Model (Resume)\")\n",
        "                    print(\"[2] Transplant Weights (Adapt)\")\n",
        "                    print(\"[3] Start Fresh\")\n",
        "                    action = input(\"Choice [1/2/3]: \").strip()\n",
        "\n",
        "                    if action == '1':\n",
        "                        print(f\"üîÑ Resizing to {saved_dim}...\")\n",
        "                        NUM_NEURONS = saved_dim\n",
        "                        model, trainer, _, _ = initialize_system(dataset.get_vocab_size(), NUM_NEURONS, DEVICE, LEARNING_RATE, ACTIVATION)\n",
        "                        opt_arg = None if RESET_OPTIMIZER_ON_LOAD else trainer.optimizer\n",
        "                        target_lr = LEARNING_RATE if OVERWRITE_LR_OF_CKPT else None\n",
        "                        load_checkpoint(model, opt_arg, CKPT_PATH, device=DEVICE, strict=True, lr=target_lr)\n",
        "                        print(f\"‚úÖ Resuming from Epoch {start_epoch}\")\n",
        "\n",
        "                    elif action == '2':\n",
        "                        print(f\"‚ö†Ô∏è Transplanting Weights...\")\n",
        "                        transplant_weights(model, CKPT_PATH, device=DEVICE)\n",
        "                        print(f\"üß¨ Transplant complete.\")\n",
        "\n",
        "                    else:\n",
        "                        print(\"üÜï Starting fresh.\")\n",
        "                        start_epoch = 0\n",
        "                        dataset.skip_offset = 0\n",
        "\n",
        "                else:\n",
        "                    opt_arg = None if RESET_OPTIMIZER_ON_LOAD else trainer.optimizer\n",
        "                    target_lr = LEARNING_RATE if OVERWRITE_LR_OF_CKPT else None\n",
        "                    print(f\"üîÑ Loading Checkpoint from {CKPT_PATH}...\")\n",
        "                    load_checkpoint(model, opt_arg, CKPT_PATH, device=DEVICE, strict=True, lr=target_lr)\n",
        "                    print(f\"‚úÖ Resuming from Epoch {start_epoch}\")\n",
        "            else:\n",
        "                 opt_arg = None if RESET_OPTIMIZER_ON_LOAD else trainer.optimizer\n",
        "                 target_lr = LEARNING_RATE if OVERWRITE_LR_OF_CKPT else None\n",
        "                 load_checkpoint(model, opt_arg, CKPT_PATH, device=DEVICE, strict=True, lr=target_lr)\n",
        "                 start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Failed to load/inspect checkpoint: {e}. Starting fresh.\")\n",
        "\n",
        "    # CrossEntropy\n",
        "    # Note: 'ignore_index' is less critical now as outputs are perfectly aligned!\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.005)\n",
        "    trainer.loss_fn = criterion\n",
        "\n",
        "    # OUTPUT TRANSFORM: Flatten (Batch, Steps, Out) -> (N, Out)\n",
        "    def flatten_logits(out):\n",
        "        return out.reshape(-1, dataset.get_vocab_size())\n",
        "\n",
        "    # --- INITIAL TESTS ---\n",
        "    print(\"\\n--- GENERATION PREVIEW ---\")\n",
        "    try:\n",
        "        gen_text = generate(model, dataset, start_str=\"The meaning of life is\", length=100)\n",
        "        print(f\"Sample: {gen_text}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    print(\"--- TRAINING LOOP ---\")\n",
        "\n",
        "    epoch = start_epoch\n",
        "    prev_loss = float('inf')\n",
        "    loss_increase_counter = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    if os.path.exists(CKPT_BEST_PATH):\n",
        "        try:\n",
        "            best_ckpt = torch.load(CKPT_BEST_PATH, map_location=DEVICE)\n",
        "            best_loss = best_ckpt.get('loss', float('inf'))\n",
        "            print(f\"üèÜ Historical Best Loss: {best_loss:.4f}\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    scheduler = None\n",
        "    if USE_SCHEDULER:\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            trainer.optimizer,\n",
        "            T_0=SCHEDULER_T0,\n",
        "            eta_min=SCHEDULER_ETA_MIN\n",
        "        )\n",
        "\n",
        "    data_iterator = iter(dataloader)\n",
        "\n",
        "    while True:\n",
        "        total_loss = 0\n",
        "        steps = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for batch_idx in range(STEPS_PER_EPOCH):\n",
        "            try:\n",
        "                x, y, current_doc_tensor = next(data_iterator)\n",
        "                current_doc = current_doc_tensor[-1].item()\n",
        "                dataset.current_doc_index = current_doc\n",
        "            except StopIteration:\n",
        "                print(\"üîÑ Restarting iterator...\")\n",
        "                data_iterator = iter(dataloader)\n",
        "                x, y, current_doc_tensor = next(data_iterator)\n",
        "                current_doc = current_doc_tensor[-1].item()\n",
        "                dataset.current_doc_index = current_doc\n",
        "\n",
        "            # Native Thinking Preparation\n",
        "            # x is raw tokens: (Batch, SeqLen)\n",
        "            # y is raw target: (Batch, SeqLen)\n",
        "            # We calculate total steps including gaps:\n",
        "            # 1 Step (Read) + N Steps (Think) = N+1 steps per token\n",
        "            x = x.to(DEVICE)\n",
        "            y = y.to(DEVICE)\n",
        "            \n",
        "            y_flat = y.reshape(-1)\n",
        "            \n",
        "            seq_len = x.shape[1]\n",
        "            total_thinking_steps = seq_len * (THINK_GAP + 1)\n",
        "\n",
        "            if TRUNCATED_BPTT_SEQ_LEN != -1 and TRUNCATED_BPTT_SEQ_LEN > 0:\n",
        "                current_state = None\n",
        "                batch_loss = 0\n",
        "                steps_count = 0\n",
        "                \n",
        "                # Chunking based on raw tokens, NOT dilated steps!\n",
        "                chunk_len = TRUNCATED_BPTT_SEQ_LEN\n",
        "                \n",
        "                for t_start in range(0, seq_len, chunk_len):\n",
        "                    t_end = min(t_start + chunk_len, seq_len)\n",
        "                    \n",
        "                    # Raw Chunk\n",
        "                    x_chunk = x[:, t_start:t_end]\n",
        "                    y_chunk_flat = y[:, t_start:t_end].reshape(-1)\n",
        "                    \n",
        "                    # Calculate thinking steps for this chunk\n",
        "                    actual_tokens = t_end - t_start\n",
        "                    chunk_thinking_steps = actual_tokens * (THINK_GAP + 1)\n",
        "\n",
        "                    loss, current_state = trainer.train_batch(\n",
        "                        x_chunk,\n",
        "                        y_chunk_flat,\n",
        "                        thinking_steps=chunk_thinking_steps,\n",
        "                        full_sequence=True,\n",
        "                        output_transform=flatten_logits,\n",
        "                        initial_state=current_state,\n",
        "                        return_state=True\n",
        "                    )\n",
        "\n",
        "                    current_state = current_state.detach()\n",
        "                    batch_loss += loss\n",
        "                    steps_count += 1\n",
        "\n",
        "                loss = batch_loss / max(steps_count, 1)\n",
        "\n",
        "            else:\n",
        "                loss = trainer.train_batch(\n",
        "                    x,\n",
        "                    y_flat,\n",
        "                    thinking_steps=total_thinking_steps,\n",
        "                    full_sequence=True,\n",
        "                    output_transform=flatten_logits\n",
        "                )\n",
        "\n",
        "            # Scheduler Step\n",
        "            current_lr = 0.0\n",
        "            if USE_SCHEDULER and scheduler:\n",
        "                scheduler.step()\n",
        "                current_lr = scheduler.get_last_lr()[0]\n",
        "            elif trainer.optimizer:\n",
        "                current_lr = trainer.optimizer.param_groups[0]['lr']\n",
        "\n",
        "            total_loss += loss\n",
        "            steps += 1\n",
        "\n",
        "            if batch_idx % LOG_INTERVAL == 0:\n",
        "                loss_val = loss.item() if isinstance(loss, torch.Tensor) else loss\n",
        "                ppl = np.exp(loss_val)\n",
        "                print(f\"Epoch {epoch} | Batch {batch_idx} | Doc #{current_doc} | Loss {loss:.4f} | PPL {ppl:.2f} | LR {current_lr:.2e}\")\n",
        "\n",
        "        avg_loss = total_loss / steps\n",
        "        avg_loss_val = avg_loss.item() if isinstance(avg_loss, torch.Tensor) else avg_loss\n",
        "        avg_ppl = np.exp(avg_loss_val)\n",
        "        print(f\"Epoch {epoch} Completed | Avg Loss: {avg_loss:.4f} | Avg PPL {avg_ppl:.2f} | Time: {time.time() - start_time:.1f}s\")\n",
        "\n",
        "        # --- PERIODIC GENERATION ---\n",
        "        print(\"--- GENERATION ---\")\n",
        "        try:\n",
        "            gen_text = generate(model, dataset, start_str=\"The meaning of life is \")\n",
        "            print(gen_text)\n",
        "        except Exception as e:\n",
        "            print(f\"Generation Error: {e}\")\n",
        "        print(\"------------------\")\n",
        "\n",
        "        # --- CHECKPOINT SAVING ---\n",
        "        ckpt_extra_data = {\n",
        "            'initial_lr': trainer.initial_lr,\n",
        "            'dataset_step': dataset.current_doc_index\n",
        "        }\n",
        "\n",
        "        save_checkpoint(model, trainer.optimizer, epoch, avg_loss, CKPT_PATH, extra_data=ckpt_extra_data)\n",
        "        print(f\"üíæ Checkpoint Saved: {CKPT_PATH} (Doc Index: {dataset.current_doc_index})\")\n",
        "\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            save_checkpoint(model, trainer.optimizer, epoch, avg_loss, CKPT_BEST_PATH, extra_data=ckpt_extra_data)\n",
        "            print(f\"üèÜ NEW RECORD! Saved: {CKPT_BEST_PATH} (Loss: {best_loss:.4f})\")\n",
        "\n",
        "        # --- NEUROGENESIS CONTROL ---\n",
        "        if NEUROGENESIS_ENABLED:\n",
        "            if avg_loss > prev_loss:\n",
        "                loss_increase_counter += 1\n",
        "                print(f\"‚ö†Ô∏è Loss Increased ({loss_increase_counter}/{MAX_LOSS_INCREASE})\")\n",
        "\n",
        "            if loss_increase_counter >= MAX_LOSS_INCREASE:\n",
        "                print(f\"üß¨ Expanding Network (Neurogenesis)...\")\n",
        "                trainer.expand(amount=NEUROGENESIS_AMOUNT)\n",
        "                NUM_NEURONS = model.num_neurons\n",
        "                loss_increase_counter = 0\n",
        "                prev_loss = float('inf')\n",
        "\n",
        "                if USE_SCHEDULER:\n",
        "                    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "                        trainer.optimizer, T_0=SCHEDULER_T0, eta_min=SCHEDULER_ETA_MIN)\n",
        "            else:\n",
        "                prev_loss = avg_loss\n",
        "\n",
        "        # --- REGENERATION CONTROL (PHOENIX) ---\n",
        "        if DARWINIAN_REGENERATION and epoch % REGENERATION_INTERVAL == 0:\n",
        "            print(f\"üî• Phoenix Protocol: Checking for dead synapses...\")\n",
        "\n",
        "            p_arg = REGENERATION_PERCENTAGE if REGENERATION_MODE == 'percentage' else None\n",
        "            t_arg = REGENERATION_THRESHOLD\n",
        "\n",
        "            revived, total = trainer.regenerate_synapses(threshold=t_arg, percentage=p_arg)\n",
        "\n",
        "            if revived > 0:\n",
        "                print(f\"üî• Reborn: {revived}/{total} ({revived/total:.2%}) synapses regenerated.\")\n",
        "                prev_loss = float('inf')\n",
        "\n",
        "        epoch += 1\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "001564ca5bad41ffa72454dabaf10015": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0aca8bf099704960a33f8b1ef88bea83": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d5c34f495834524a1a077b67fa35ee7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e3ec207041b4ee09557f5027401e4a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e48976ffbd1499c929d2430f9f5d36d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a967241441664c6a894833737472068c",
            "value": "Resolving‚Äádata‚Äáfiles:‚Äá100%"
          }
        },
        "24aeaa3ab0fc4110a839eca4ad2bdd8f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27bddea9e66e447bad34eed9d9af84e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4480b16cb46242d68dd98d5fb13355d6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b2dba098822d406a94971f039bec6eea",
            "value": "‚Äá26.4k/?‚Äá[00:00&lt;00:00,‚Äá2.82MB/s]"
          }
        },
        "3cba134770e14193b57a4b03a6879ace": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42e01134b61948508d8fdeb540ab3a07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4480b16cb46242d68dd98d5fb13355d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46b7e7e6516243a99be1ec6b3ecc28ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e48976ffbd1499c929d2430f9f5d36d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fe44d116771413086fb53d506c66ae3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9df4e339a22d492f9a4f4967affdbf97",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_46b7e7e6516243a99be1ec6b3ecc28ae",
            "value": "README.md:‚Äá"
          }
        },
        "66510255473747c39f0ca25c949889cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6dbbe4a819ef48598ab9a07cd1a3bf54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6e76caf009a249bf9d8316329b7e2518": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f11cb5c9dc474353b17c776372af38f0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bd22b2a7616f4c9e9463dc1ab0f7bd6e",
            "value": "Resolving‚Äádata‚Äáfiles:‚Äá100%"
          }
        },
        "7b76f4aee386460bab874dcde2c68731": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2af97eeb1124f248c6a099803e7ff86",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_001564ca5bad41ffa72454dabaf10015",
            "value": "‚Äá50/50‚Äá[00:00&lt;00:00,‚Äá6163.92it/s]"
          }
        },
        "9df4e339a22d492f9a4f4967affdbf97": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a967241441664c6a894833737472068c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b233baea8150447daf079c651075c504": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2dba098822d406a94971f039bec6eea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3b402f5c0d444c5a1e900c765703188": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4f9197effdd4b309f4642c4d7790226": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edf9dedf22bd451097dee8d13827f024",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6dbbe4a819ef48598ab9a07cd1a3bf54",
            "value": 1
          }
        },
        "b91b9b0026d64f838c5bb739f4544105": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5fe44d116771413086fb53d506c66ae3",
              "IPY_MODEL_b4f9197effdd4b309f4642c4d7790226",
              "IPY_MODEL_27bddea9e66e447bad34eed9d9af84e9"
            ],
            "layout": "IPY_MODEL_3cba134770e14193b57a4b03a6879ace"
          }
        },
        "bd22b2a7616f4c9e9463dc1ab0f7bd6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2af97eeb1124f248c6a099803e7ff86": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7e8c88f568440e997a795badb70657c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0e3cba64b0147a9bcbc56961f48727e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_42e01134b61948508d8fdeb540ab3a07",
            "value": "‚Äá2410/2410‚Äá[00:00&lt;00:00,‚Äá‚Äá7.10it/s]"
          }
        },
        "e1887135234b4cf4915691160febfa82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0aca8bf099704960a33f8b1ef88bea83",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66510255473747c39f0ca25c949889cb",
            "value": 50
          }
        },
        "ebe90f94272341028a42036b48718e9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e3ec207041b4ee09557f5027401e4a6",
              "IPY_MODEL_efe5028be49745f49ac232357608b09f",
              "IPY_MODEL_d7e8c88f568440e997a795badb70657c"
            ],
            "layout": "IPY_MODEL_b3b402f5c0d444c5a1e900c765703188"
          }
        },
        "edf9dedf22bd451097dee8d13827f024": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "efe5028be49745f49ac232357608b09f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24aeaa3ab0fc4110a839eca4ad2bdd8f",
            "max": 2410,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d5c34f495834524a1a077b67fa35ee7",
            "value": 2410
          }
        },
        "f0e3cba64b0147a9bcbc56961f48727e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f11cb5c9dc474353b17c776372af38f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5b63ee58f744648908003d21033e61b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e76caf009a249bf9d8316329b7e2518",
              "IPY_MODEL_e1887135234b4cf4915691160febfa82",
              "IPY_MODEL_7b76f4aee386460bab874dcde2c68731"
            ],
            "layout": "IPY_MODEL_b233baea8150447daf079c651075c504"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
